{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1M9mhKP_0itNFq43EXlzdI96rAzo2tYSC",
      "authorship_tag": "ABX9TyOPjEW8I9QPua4V0cma8/I9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sri-Deepthi-N/CAD_Phase1/blob/main/SHMS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5igWvKGWgkJs"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Step 1: Load the dataset\n",
        "file_path = '/content/heart_disease.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Step 2: Preprocess the data\n",
        "# Drop the target column if present (unsupervised anomaly detection)\n",
        "if 'Heart Disease Status' in data.columns:\n",
        "    X = data.drop(columns=['Heart Disease Status'])\n",
        "else:\n",
        "    X = data\n",
        "\n",
        "# Handle missing values using mean imputation\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X_imputed = imputer.fit_transform(X)\n",
        "\n",
        "# Normalize the data using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_imputed)\n",
        "\n",
        "# Step 3: Train the Isolation Forest model\n",
        "model = IsolationForest(n_estimators=100, contamination=0.1, random_state=42)\n",
        "model.fit(X_scaled)\n",
        "\n",
        "# Step 4: Predict anomalies\n",
        "# '-1' indicates anomalies, '1' indicates normal data\n",
        "anomaly_predictions = model.predict(X_scaled)\n",
        "\n",
        "# Convert predictions to binary labels (0: normal, 1: anomaly)\n",
        "data['Anomaly'] = (anomaly_predictions == -1).astype(int)\n",
        "\n",
        "# Step 5: Print heart rate for detected anomalies\n",
        "if 'CRP Level' in data.columns:\n",
        "    print(\"CRP Levels of Detected Anomalies:\")\n",
        "    anomalies = data[data['Anomaly'] == 1]\n",
        "    for index, row in anomalies.iterrows():\n",
        "        print(f\"Anomaly detected! CRP Level: {row['CRP Level']}\")\n",
        "else:\n",
        "    print(\"The 'CRP Level' column was not found in the dataset.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the data\n",
        "data = pd.read_csv('/content/heart_disease.csv')\n",
        "\n",
        "# Select a single feature for time-series prediction (e.g., Blood Pressure)\n",
        "time_series_data = data['Blood Pressure'].dropna().values.reshape(-1, 1)\n",
        "\n",
        "# Scale the data\n",
        "scaler = MinMaxScaler()\n",
        "time_series_data = scaler.fit_transform(time_series_data)\n",
        "\n",
        "# Create time-series sequences for LSTM\n",
        "lookback = 5\n",
        "X, y = [], []\n",
        "for i in range(len(time_series_data) - lookback):\n",
        "    X.append(time_series_data[i:i + lookback])\n",
        "    y.append(time_series_data[i + lookback])\n",
        "\n",
        "X, y = np.array(X), np.array(y)\n",
        "\n",
        "# Split the data into 80% train and 20% test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
        "\n",
        "# Define LSTM model\n",
        "model = Sequential([\n",
        "    LSTM(50, activation='relu', input_shape=(lookback, 1)),\n",
        "    Dense(1)  # Predict the next value\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=5, batch_size=32, verbose=1)\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "test_loss = model.evaluate(X_test, y_test, verbose=1)\n",
        "print(f\"Test Loss: {test_loss}\")\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "# Inverse transform predictions and test labels\n",
        "predictions = scaler.inverse_transform(predictions)\n",
        "y_test_inv = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
        "\n",
        "# Results comparison\n",
        "results = pd.DataFrame({'Actual': y_test_inv.flatten(), 'Predicted': predictions.flatten()})\n",
        "print(results.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kobwN4Q0qmUy",
        "outputId": "89dfa15a-3878-445d-839e-4e4827e13c4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.1472\n",
            "Epoch 2/5\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0873\n",
            "Epoch 3/5\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0875\n",
            "Epoch 4/5\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0870\n",
            "Epoch 5/5\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0867\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0862\n",
            "Test Loss: 0.08494746685028076\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
            "   Actual   Predicted\n",
            "0   133.0  150.201752\n",
            "1   178.0  149.754837\n",
            "2   166.0  150.052643\n",
            "3   162.0  150.620895\n",
            "4   175.0  150.570694\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "import pandas as pd\n",
        "\n",
        "# Load the data\n",
        "data = pd.read_csv('/content/heart_disease.csv')\n",
        "\n",
        "# Ensure column names are clean\n",
        "data.columns = data.columns.str.strip()\n",
        "\n",
        "# Identify the target column and features\n",
        "target_column = \"Heart Disease Status\"  # Target column\n",
        "X = data.drop(columns=[target_column]).fillna(0)  # Features\n",
        "y = data[target_column]  # Target\n",
        "\n",
        "# Encode the target column if it is categorical\n",
        "if y.dtype == 'object':\n",
        "    y = y.astype('category').cat.codes\n",
        "\n",
        "# Check class distribution\n",
        "print(\"Class Distribution:\\n\", y.value_counts())\n",
        "\n",
        "# Split the data into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features (scaling to zero mean and unit variance)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Initialize and train the SVM model with class balancing\n",
        "svm_model = SVC(kernel='rbf', C=1.0, gamma='scale', class_weight='balanced', random_state=42)\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = svm_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred, zero_division=0))\n",
        "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# Save the predictions\n",
        "results = pd.DataFrame({\n",
        "    'Actual': y_test,\n",
        "    'Predicted': y_pred\n",
        "})"
      ],
      "metadata": {
        "id": "EJycbx1Mqr8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('/content/heart_disease.csv')\n",
        "\n",
        "# Preprocessing: Handle missing values, if any\n",
        "data = data.dropna()\n",
        "\n",
        "# Select features for anomaly detection (exclude target column if present)\n",
        "features = data.drop(columns=['target'], errors='ignore')  # Replace 'target' with actual target column name if applicable\n",
        "\n",
        "# Normalize features\n",
        "scaler = StandardScaler()\n",
        "scaled_features = scaler.fit_transform(features)\n",
        "\n",
        "# Split the data for training and testing (80% for training, 20% for testing)\n",
        "X_train, X_test = train_test_split(scaled_features, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Isolation Forest\n",
        "model = IsolationForest(n_estimators=100, contamination=0.1, random_state=42)\n",
        "model.fit(X_train)\n",
        "\n",
        "# Predict anomalies for the test set (-1: anomaly, 1: normal)\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "# Add predictions back to the data for analysis\n",
        "test_results = pd.DataFrame(X_test, columns=features.columns)\n",
        "test_results['Anomaly'] = predictions\n",
        "\n",
        "\n",
        "# If 'CRP Level' column is present, print CRP Levels of detected anomalies\n",
        "if 'CRP Level' in data.columns:\n",
        "    print(\"\\nCRP Levels of Detected Anomalies:\")\n",
        "    anomalies = test_results[test_results['Anomaly'] == -1]  # Anomalies are labeled as -1\n",
        "    for index, row in anomalies.iterrows():\n",
        "        print(f\"Anomaly detected! CRP Level: {row['CRP Level']}\")\n",
        "else:\n",
        "    print(\"The 'CRP Level' column was not found in the dataset.\")\n"
      ],
      "metadata": {
        "id": "piVsyuSADEXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Load your dataset from CSV\n",
        "data = pd.read_csv('heart_disease.csv')\n",
        "\n",
        "# Ensure there are no NaN or infinite values in the data\n",
        "data = data.replace([np.inf, -np.inf], np.nan)\n",
        "data = data.dropna()\n",
        "\n",
        "# Step 1: Split the data into training and testing (80% for training, 20% for testing)\n",
        "train_data, test_data = train_test_split(data.values, test_size=0.2, random_state=42)\n",
        "\n",
        "# Normalize the data (use MinMaxScaler to scale the data to a range of [0,1])\n",
        "scaler = MinMaxScaler()\n",
        "train_data_scaled = scaler.fit_transform(train_data)\n",
        "test_data_scaled = scaler.transform(test_data)\n",
        "\n",
        "# Step 2: Define the Autoencoder model\n",
        "input_dim = train_data_scaled.shape[1]\n",
        "\n",
        "input_layer = Input(shape=(input_dim,))\n",
        "encoded = Dense(16, activation='relu')(input_layer)\n",
        "encoded = Dense(8, activation='relu')(encoded)\n",
        "decoded = Dense(16, activation='relu')(encoded)\n",
        "decoded = Dense(input_dim, activation='linear')(decoded)  # Changed from 'sigmoid' to 'linear'\n",
        "\n",
        "autoencoder = Model(inputs=input_layer, outputs=decoded)\n",
        "autoencoder.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Step 3: Train the model\n",
        "autoencoder.fit(train_data_scaled, train_data_scaled,\n",
        "                epochs=5,\n",
        "                batch_size=32,\n",
        "                shuffle=True,\n",
        "                validation_data=(test_data_scaled, test_data_scaled))\n",
        "\n",
        "# Step 4: Evaluate on test data\n",
        "# Calculate reconstruction errors\n",
        "reconstructed = autoencoder.predict(test_data_scaled)\n",
        "reconstruction_errors = np.mean(np.square(test_data_scaled - reconstructed), axis=1)\n",
        "\n",
        "# Step 5: Set a threshold for anomalies\n",
        "threshold = np.percentile(reconstruction_errors, 95)  # Adjust based on requirements\n",
        "\n",
        "# Flag anomalies (1 = anomaly, 0 = normal)\n",
        "anomalies = reconstruction_errors > threshold\n",
        "\n",
        "# Assume you have ground truth labels for anomalies in `ground_truth_labels` (1 for anomaly, 0 for normal)\n",
        "# Replace this with your actual labels\n",
        "ground_truth_labels = np.random.choice([0, 1], size=len(test_data))  # Example ground truth labels\n",
        "\n",
        "# Example: Inverse transform the predictions and actual data to the original scale\n",
        "predictions = autoencoder.predict(test_data_scaled)\n",
        "predictions = scaler.inverse_transform(predictions)\n",
        "\n",
        "actual_values = scaler.inverse_transform(test_data)\n",
        "\n",
        "# Create a DataFrame for displaying Actual vs Predicted values\n",
        "results = pd.DataFrame({\n",
        "    'Actual': actual_values.flatten(),  # Flatten the actual data for easy comparison\n",
        "    'Predicted': predictions.flatten()   # Flatten the predictions for easy comparison\n",
        "})\n",
        "\n",
        "print(results.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2dn6pWzWFTxj",
        "outputId": "5c58be9d-b201-4dd0-c278-fe854b2cfb14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.2487 - val_loss: 0.1351\n",
            "Epoch 2/5\n",
            "\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1314 - val_loss: 0.1214\n",
            "Epoch 3/5\n",
            "\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1191 - val_loss: 0.1102\n",
            "Epoch 4/5\n",
            "\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1090 - val_loss: 0.1056\n",
            "Epoch 5/5\n",
            "\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1045 - val_loss: 0.1033\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
            "    Actual   Predicted\n",
            "0   2622.0   54.059902\n",
            "1      2.0    1.306432\n",
            "2  10080.0  150.822052\n",
            "3  33450.0  227.518082\n",
            "4      5.0    2.106972\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the dataset\n",
        "file_path = \"heart_disease.csv\"  # Change path if needed\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Step 2: Data Preprocessing\n",
        "df_cleaned = df.dropna()  # Drop missing values\n",
        "\n",
        "# Separate features and target variable\n",
        "X = df_cleaned.drop(columns=[\"Heart Disease Status\"])\n",
        "y = df_cleaned[\"Heart Disease Status\"]\n",
        "\n",
        "# Normalize numerical features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split data into training (80%) and testing (20%)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Apply Isolation Forest for Anomaly Detection\n",
        "iso_forest = IsolationForest(n_estimators=100, contamination=0.05, random_state=42)\n",
        "anomaly_scores = iso_forest.fit_predict(X_train)\n",
        "\n",
        "# Identify anomalies (outliers are labeled as -1)\n",
        "anomalies = np.where(anomaly_scores == -1)[0]\n",
        "\n",
        "# Step 4: Train XGBoost Classifier\n",
        "clf = XGBClassifier(eval_metric=\"logloss\")\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Evaluate the model\n",
        "y_pred = clf.predict(X_test)\n",
        "overall_accuracy = accuracy_score(y_test, y_pred)  # No need to multiply by 100\n",
        "print(f\"Overall Model Accuracy: {overall_accuracy:.2f}\")\n",
        "\n",
        "# Step 6: Compute accuracy and loss for each anomaly\n",
        "print(\"\\nAccuracy and Loss for Each Anomaly:\")\n",
        "all_anomalies_X = X_train[anomalies]  # Get all anomalies\n",
        "all_anomalies_y = y_train.iloc[anomalies]  # Get actual labels\n",
        "\n",
        "# Predict all anomalies\n",
        "all_anomalies_pred = clf.predict(all_anomalies_X)\n",
        "\n",
        "# Compute overall anomaly accuracy\n",
        "anomaly_accuracy = accuracy_score(all_anomalies_y, all_anomalies_pred)  # No need to multiply by 100\n",
        "accuracy_loss = overall_accuracy - anomaly_accuracy  # Compute accuracy loss\n",
        "\n",
        "# Print each anomaly's accuracy and loss\n",
        "for i, idx in enumerate(anomalies):\n",
        "    feature_values = X_train[idx]  # Get feature values of the anomaly\n",
        "    predicted_label = all_anomalies_pred[i]  # Get predicted label\n",
        "    actual_label = all_anomalies_y.iloc[i]  # Get actual label\n",
        "\n",
        "    # Calculate accuracy for each anomaly\n",
        "    anomaly_accuracy_for_this = 1.0 if predicted_label == actual_label else 0.0\n",
        "\n",
        "    # Loss is the difference from overall accuracy\n",
        "    anomaly_loss_for_this = overall_accuracy - anomaly_accuracy_for_this\n",
        "\n",
        "    print(f\"Anomaly {i+1}: Accuracy = {anomaly_accuracy_for_this:.2f}, Loss = {anomaly_loss_for_this:.2f}\")\n",
        "\n",
        "# Print final summary\n",
        "print(f\"\\nOverall Anomaly Detection Accuracy: {anomaly_accuracy:.2f}\")\n",
        "print(f\"Overall Accuracy Loss Due to Anomalies: {accuracy_loss:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O3wroIzAYZzG",
        "outputId": "69c6148b-447f-4f3e-a39e-54915bc4ecc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overall Model Accuracy: 0.79\n",
            "\n",
            "Accuracy and Loss for Each Anomaly:\n",
            "Anomaly 1: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 2: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 3: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 4: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 5: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 6: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 7: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 8: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 9: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 10: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 11: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 12: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 13: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 14: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 15: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 16: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 17: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 18: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 19: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 20: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 21: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 22: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 23: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 24: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 25: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 26: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 27: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 28: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 29: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 30: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 31: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 32: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 33: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 34: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 35: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 36: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 37: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 38: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 39: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 40: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 41: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 42: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 43: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 44: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 45: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 46: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 47: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 48: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 49: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 50: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 51: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 52: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 53: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 54: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 55: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 56: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 57: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 58: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 59: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 60: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 61: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 62: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 63: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 64: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 65: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 66: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 67: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 68: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 69: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 70: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 71: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 72: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 73: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 74: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 75: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 76: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 77: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 78: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 79: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 80: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 81: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 82: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 83: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 84: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 85: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 86: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 87: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 88: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 89: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 90: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 91: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 92: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 93: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 94: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 95: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 96: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 97: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 98: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 99: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 100: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 101: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 102: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 103: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 104: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 105: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 106: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 107: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 108: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 109: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 110: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 111: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 112: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 113: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 114: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 115: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 116: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 117: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 118: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 119: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 120: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 121: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 122: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 123: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 124: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 125: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 126: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 127: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 128: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 129: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 130: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 131: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 132: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 133: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 134: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 135: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 136: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 137: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 138: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 139: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 140: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 141: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 142: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 143: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 144: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 145: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 146: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 147: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 148: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 149: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 150: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 151: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 152: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 153: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 154: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 155: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 156: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 157: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 158: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 159: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 160: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 161: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 162: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 163: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 164: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 165: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 166: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 167: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 168: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 169: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 170: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 171: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 172: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 173: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 174: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 175: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 176: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 177: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 178: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 179: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 180: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 181: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 182: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 183: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 184: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 185: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 186: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 187: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 188: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 189: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 190: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 191: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 192: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 193: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 194: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 195: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 196: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 197: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 198: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 199: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 200: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 201: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 202: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 203: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 204: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 205: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 206: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 207: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 208: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 209: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 210: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 211: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 212: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 213: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 214: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 215: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 216: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 217: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 218: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 219: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 220: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 221: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 222: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 223: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 224: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 225: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 226: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 227: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 228: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 229: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 230: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 231: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 232: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 233: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 234: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 235: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 236: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 237: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 238: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 239: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 240: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 241: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 242: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 243: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 244: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 245: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 246: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 247: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 248: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 249: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 250: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 251: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 252: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 253: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 254: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 255: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 256: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 257: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 258: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 259: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 260: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 261: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 262: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 263: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 264: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 265: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 266: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 267: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 268: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 269: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 270: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 271: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 272: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 273: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 274: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 275: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 276: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 277: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 278: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 279: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 280: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 281: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 282: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 283: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 284: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 285: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 286: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 287: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 288: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 289: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 290: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 291: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 292: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 293: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 294: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 295: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 296: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 297: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 298: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 299: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 300: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 301: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 302: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 303: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 304: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 305: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 306: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 307: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 308: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 309: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 310: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 311: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 312: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 313: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 314: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 315: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 316: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 317: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 318: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 319: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 320: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 321: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 322: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 323: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 324: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 325: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 326: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 327: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 328: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 329: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 330: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 331: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 332: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 333: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 334: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 335: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 336: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 337: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 338: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 339: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 340: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 341: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 342: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 343: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 344: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 345: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 346: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 347: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 348: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 349: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 350: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 351: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 352: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 353: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 354: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 355: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 356: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 357: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 358: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 359: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 360: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 361: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 362: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 363: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 364: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 365: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 366: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 367: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 368: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 369: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 370: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 371: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 372: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 373: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 374: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 375: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 376: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 377: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 378: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 379: Accuracy = 1.00, Loss = -0.21\n",
            "Anomaly 380: Accuracy = 1.00, Loss = -0.21\n",
            "\n",
            "Overall Anomaly Detection Accuracy: 1.00\n",
            "Overall Accuracy Loss Due to Anomalies: -0.21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"xgboost\")\n",
        "\n",
        "# Step 1: Load the dataset\n",
        "file_path = \"Dataset Heart Disease.csv\"  # Change path if needed\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Drop unnecessary columns\n",
        "df_cleaned = df.drop(columns=[\"Unnamed: 0\"])\n",
        "\n",
        "# Define features and target\n",
        "X = df_cleaned.drop(columns=[\"target\"])\n",
        "y = df_cleaned[\"target\"].astype(int)  # Ensure y is an integer\n",
        "\n",
        "# Identify numerical columns for scaling\n",
        "num_cols = [\"age\", \"resting bps\", \"cholesterol\", \"max heart rate\", \"oldpeak\"]\n",
        "\n",
        "# Scale numerical features only\n",
        "scaler = StandardScaler()\n",
        "X[num_cols] = scaler.fit_transform(X[num_cols])\n",
        "\n",
        "# Step 2: Split dataset into training (80%) and testing (20%)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert data to NumPy arrays for XGBoost compatibility\n",
        "X_train, X_test, y_train, y_test = X_train.values, X_test.values, y_train.values, y_test.values\n",
        "\n",
        "# Step 3: Apply Isolation Forest for Anomaly Detection\n",
        "iso_forest = IsolationForest(n_estimators=200, contamination=0.05, random_state=42)\n",
        "anomaly_scores = iso_forest.fit_predict(X_train)\n",
        "\n",
        "# Identify anomalies (outliers are labeled as -1)\n",
        "anomalies = np.where(anomaly_scores == -1)[0]\n",
        "\n",
        "# Remove detected anomalies from training data to improve accuracy\n",
        "X_train_clean = np.delete(X_train, anomalies, axis=0)\n",
        "y_train_clean = np.delete(y_train, anomalies, axis=0)\n",
        "\n",
        "# Step 4: Train XGBoost Classifier with Optimized Hyperparameters\n",
        "clf = XGBClassifier(n_estimators=200, learning_rate=0.05, max_depth=6, eval_metric=\"logloss\", use_label_encoder=False)\n",
        "\n",
        "clf.fit(X_train_clean, y_train_clean)\n",
        "\n",
        "# Step 5: Evaluate the model\n",
        "y_pred = clf.predict(X_test)\n",
        "overall_accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Overall Model Accuracy (After Removing Anomalies): {overall_accuracy:.2f}\")\n",
        "\n",
        "# Step 6: Compute accuracy and loss for anomalies\n",
        "if len(anomalies) > 0:\n",
        "    print(f\"\\nTotal number of anomalies detected: {len(anomalies)}\")\n",
        "\n",
        "    # Get all anomaly data\n",
        "    all_anomalies_X = X_train[anomalies]\n",
        "    all_anomalies_y = y_train[anomalies]\n",
        "\n",
        "    # Predict anomalies\n",
        "    all_anomalies_pred = clf.predict(all_anomalies_X)\n",
        "\n",
        "    # Compute overall anomaly accuracy\n",
        "    anomaly_accuracy = accuracy_score(all_anomalies_y, all_anomalies_pred)\n",
        "    accuracy_loss = overall_accuracy - anomaly_accuracy\n",
        "\n",
        "    # Print each anomaly’s accuracy and loss\n",
        "    print(\"\\nAnomaly Details and Accuracy Loss:\")\n",
        "    for i, idx in enumerate(anomalies):\n",
        "        predicted_label = all_anomalies_pred[i]\n",
        "        actual_label = all_anomalies_y[i]\n",
        "\n",
        "        # Accuracy per anomaly\n",
        "        anomaly_accuracy_for_this = 1.0 if predicted_label == actual_label else 0.0\n",
        "        anomaly_loss_for_this = overall_accuracy - anomaly_accuracy_for_this\n",
        "\n",
        "        print(f\"Anomaly {i+1}:  \"\n",
        "              f\"Accuracy = {anomaly_accuracy_for_this:.2f}, Loss = {anomaly_loss_for_this:.2f}\")\n",
        "\n",
        "    # Display all detected anomalies\n",
        "    anomalies_df = pd.DataFrame(X_train[anomalies], columns=X.columns)\n",
        "    anomalies_df[\"Actual Target\"] = all_anomalies_y\n",
        "    anomalies_df[\"Predicted Target\"] = all_anomalies_pred\n",
        "\n",
        "    # Print final anomaly detection summary\n",
        "    print(f\"\\nOverall Anomaly Detection Accuracy: {anomaly_accuracy:.2f}\")\n",
        "    print(f\"Overall Accuracy Loss Due to Anomalies: {accuracy_loss:.2f}\")\n",
        "else:\n",
        "    print(\"\\nNo anomalies detected in the dataset.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQiLYwmbc4tw",
        "outputId": "25cbb261-244e-48dd-94f9-620ffca140f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overall Model Accuracy (After Removing Anomalies): 0.73\n",
            "\n",
            "Total number of anomalies detected: 42\n",
            "\n",
            "Anomaly Details and Accuracy Loss:\n",
            "Anomaly 1:  Accuracy = 1.00, Loss = -0.27\n",
            "Anomaly 2:  Accuracy = 0.00, Loss = 0.73\n",
            "Anomaly 3:  Accuracy = 1.00, Loss = -0.27\n",
            "Anomaly 4:  Accuracy = 1.00, Loss = -0.27\n",
            "Anomaly 5:  Accuracy = 1.00, Loss = -0.27\n",
            "Anomaly 6:  Accuracy = 0.00, Loss = 0.73\n",
            "Anomaly 7:  Accuracy = 0.00, Loss = 0.73\n",
            "Anomaly 8:  Accuracy = 1.00, Loss = -0.27\n",
            "Anomaly 9:  Accuracy = 0.00, Loss = 0.73\n",
            "Anomaly 10:  Accuracy = 1.00, Loss = -0.27\n",
            "Anomaly 11:  Accuracy = 1.00, Loss = -0.27\n",
            "Anomaly 12:  Accuracy = 1.00, Loss = -0.27\n",
            "Anomaly 13:  Accuracy = 1.00, Loss = -0.27\n",
            "Anomaly 14:  Accuracy = 1.00, Loss = -0.27\n",
            "Anomaly 15:  Accuracy = 1.00, Loss = -0.27\n",
            "Anomaly 16:  Accuracy = 1.00, Loss = -0.27\n",
            "Anomaly 17:  Accuracy = 1.00, Loss = -0.27\n",
            "Anomaly 18:  Accuracy = 1.00, Loss = -0.27\n",
            "Anomaly 19:  Accuracy = 1.00, Loss = -0.27\n",
            "Anomaly 20:  Accuracy = 1.00, Loss = -0.27\n",
            "Anomaly 21:  Accuracy = 1.00, Loss = -0.27\n",
            "Anomaly 22:  Accuracy = 1.00, Loss = -0.27\n",
            "Anomaly 23:  Accuracy = 1.00, Loss = -0.27\n",
            "Anomaly 24:  Accuracy = 0.00, Loss = 0.73\n",
            "Anomaly 25:  Accuracy = 1.00, Loss = -0.27\n",
            "Anomaly 26:  Accuracy = 1.00, Loss = -0.27\n",
            "Anomaly 27:  Accuracy = 0.00, Loss = 0.73\n",
            "Anomaly 28:  Accuracy = 1.00, Loss = -0.27\n",
            "Anomaly 29:  Accuracy = 1.00, Loss = -0.27\n",
            "Anomaly 30:  Accuracy = 1.00, Loss = -0.27\n",
            "Anomaly 31:  Accuracy = 1.00, Loss = -0.27\n",
            "Anomaly 32:  Accuracy = 1.00, Loss = -0.27\n",
            "Anomaly 33:  Accuracy = 1.00, Loss = -0.27\n",
            "Anomaly 34:  Accuracy = 1.00, Loss = -0.27\n",
            "Anomaly 35:  Accuracy = 1.00, Loss = -0.27\n",
            "Anomaly 36:  Accuracy = 1.00, Loss = -0.27\n",
            "Anomaly 37:  Accuracy = 1.00, Loss = -0.27\n",
            "Anomaly 38:  Accuracy = 1.00, Loss = -0.27\n",
            "Anomaly 39:  Accuracy = 0.00, Loss = 0.73\n",
            "Anomaly 40:  Accuracy = 1.00, Loss = -0.27\n",
            "Anomaly 41:  Accuracy = 1.00, Loss = -0.27\n",
            "Anomaly 42:  Accuracy = 0.00, Loss = 0.73\n",
            "\n",
            "Overall Anomaly Detection Accuracy: 0.81\n",
            "Overall Accuracy Loss Due to Anomalies: -0.08\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the dataset\n",
        "file_path = \"Dataset Heart Disease.csv\"  # Adjust path if needed\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Drop unnecessary columns\n",
        "df_cleaned = df.drop(columns=[\"Unnamed: 0\"])\n",
        "\n",
        "# Define features and target\n",
        "X = df_cleaned.drop(columns=[\"target\"])\n",
        "y = df_cleaned[\"target\"].astype(int)  # Ensure target is an integer\n",
        "\n",
        "# Identify numerical columns for scaling\n",
        "num_cols = [\"age\", \"resting bps\", \"cholesterol\", \"max heart rate\", \"oldpeak\"]\n",
        "\n",
        "# Scale numerical features\n",
        "scaler = StandardScaler()\n",
        "X[num_cols] = scaler.fit_transform(X[num_cols])\n",
        "\n",
        "# Step 2: Split dataset into training (80%) and testing (20%)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Train an XGBoost classifier\n",
        "xgb_clf = XGBClassifier(n_estimators=200, learning_rate=0.05, max_depth=6, eval_metric=\"logloss\", use_label_encoder=False)\n",
        "xgb_clf.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Predict on the test set\n",
        "y_pred = xgb_clf.predict(X_test)\n",
        "\n",
        "# Step 5: Compute overall accuracy and loss\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "accuracy_loss = 1 - accuracy  # Loss is 1 - accuracy\n",
        "\n",
        "# Print overall accuracy and loss with high precision\n",
        "print(f\"Model Accuracy: {accuracy:.5f}\")\n",
        "print(f\"Model Loss: {accuracy_loss:.5f}\")\n",
        "\n",
        "# Step 6: Disease mapping (adjust based on your dataset)\n",
        "# Mapping numeric target values to actual disease names\n",
        "disease_mapping = {\n",
        "    0: \"Heart Attack\",\n",
        "    1: \"Heart Failure\",\n",
        "    2: \"Congenital Heart Disease\",\n",
        "    3: \"Arrhythmia\"  # Add any other disease types if necessary\n",
        "}\n",
        "\n",
        "# Map actual and predicted target values to disease names\n",
        "y_test_mapped = [disease_mapping[val] for val in y_test]\n",
        "y_pred_mapped = [disease_mapping[val] for val in y_pred]\n",
        "\n",
        "# Step 7: Calculate accuracy and loss for each row\n",
        "accuracy_per_row = [1 if actual == predicted else 0 for actual, predicted in zip(y_test, y_pred)]\n",
        "loss_per_row = [1 - acc for acc in accuracy_per_row]\n",
        "\n",
        "# Create a DataFrame for disease names with corresponding accuracy/loss\n",
        "results_df = pd.DataFrame({\n",
        "    'Disease Type': y_pred_mapped,  # Showing predicted disease name\n",
        "    'Accuracy Rate': accuracy_per_row,\n",
        "    'Loss Rate': loss_per_row\n",
        "})\n",
        "\n",
        "# Print all rows without rounding off accuracy and loss rates\n",
        "pd.set_option('display.float_format', '{:.5f}'.format)  # Set precision for display\n",
        "\n",
        "print(\"\\nAll Test Cases with Disease Types, Accuracy, and Loss Rates:\")\n",
        "print(results_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RMywb6zlibyn",
        "outputId": "d4a1d59f-97c1-4665-bbee-c7b85e9a8bd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [03:28:10] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 0.74286\n",
            "Model Loss: 0.25714\n",
            "\n",
            "All Test Cases with Disease Types, Accuracy, and Loss Rates:\n",
            "      Disease Type  Accuracy Rate  Loss Rate\n",
            "0     Heart Attack              0          1\n",
            "1     Heart Attack              0          1\n",
            "2    Heart Failure              0          1\n",
            "3    Heart Failure              0          1\n",
            "4     Heart Attack              1          0\n",
            "..             ...            ...        ...\n",
            "205  Heart Failure              1          0\n",
            "206  Heart Failure              1          0\n",
            "207  Heart Failure              1          0\n",
            "208  Heart Failure              1          0\n",
            "209  Heart Failure              1          0\n",
            "\n",
            "[210 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('/content/Dataset Heart Disease.csv')\n",
        "\n",
        "# Print the column names to ensure correct referencing\n",
        "print(df.columns)\n",
        "\n",
        "# Normalize the features (replace these with your actual column names)\n",
        "features = [\"age\", \"resting bps\", \"cholesterol\", \"max heart rate\", \"oldpeak\"]\n",
        "target = 'target'  # Target column for prediction\n",
        "\n",
        "# Normalize the features and the target separately\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "scaled_features = scaler.fit_transform(df[features])\n",
        "\n",
        "# Normalize the target separately\n",
        "scaler_target = MinMaxScaler(feature_range=(0, 1))\n",
        "scaled_target = scaler_target.fit_transform(df[[target]])\n",
        "\n",
        "# Prepare data for LSTM (assuming the time series data is already ordered)\n",
        "look_back = 30  # Number of previous time steps to look at (e.g., last 30 time steps)\n",
        "\n",
        "# Function to create datasets for LSTM\n",
        "def create_dataset(data, target, look_back):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - look_back):\n",
        "        X.append(data[i:(i + look_back)])\n",
        "        y.append(target[i + look_back])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "X, y = create_dataset(scaled_features, scaled_target, look_back)\n",
        "\n",
        "# Reshape the data for LSTM input (samples, time steps, features)\n",
        "X = X.reshape(X.shape[0], X.shape[1], X.shape[2])\n",
        "\n",
        "# Step 2: Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
        "\n",
        "# Step 3: Build the LSTM Model\n",
        "model = Sequential()\n",
        "model.add(LSTM(50, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
        "model.add(LSTM(50, return_sequences=False))\n",
        "model.add(Dense(25))\n",
        "model.add(Dense(1))  # Predicting one value (e.g., target)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Step 4: Train the Model\n",
        "model.fit(X_train, y_train, epochs=20, batch_size=32)\n",
        "\n",
        "# Step 5: Evaluate the Model\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "# Inverse transform the predictions (only for the target)\n",
        "predictions = scaler_target.inverse_transform(predictions)\n",
        "\n",
        "# Inverse transform the actual values (only for the target)\n",
        "y_test_actual = scaler_target.inverse_transform(y_test)\n",
        "\n",
        "# Example: Calculate Mean Absolute Error\n",
        "mae = mean_absolute_error(y_test_actual, predictions)\n",
        "print(f'Mean Absolute Error: {mae}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "inio8bESnDHI",
        "outputId": "80dcde77-7f0c-4305-82e6-9ff0aa574ded"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['Unnamed: 0', 'age', 'sex', 'chest pain type', 'resting bps',\n",
            "       'cholesterol', 'fasting blood sugar', 'resting ecg', 'max heart rate',\n",
            "       'exercise angina', 'oldpeak', 'ST slope', 'target'],\n",
            "      dtype='object')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 27ms/step - loss: 0.3016\n",
            "Epoch 2/20\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.2444\n",
            "Epoch 3/20\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.2440\n",
            "Epoch 4/20\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.2394\n",
            "Epoch 5/20\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.2433\n",
            "Epoch 6/20\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.2408\n",
            "Epoch 7/20\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.2392\n",
            "Epoch 8/20\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.2489\n",
            "Epoch 9/20\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.2428\n",
            "Epoch 10/20\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 0.2399\n",
            "Epoch 11/20\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 41ms/step - loss: 0.2466\n",
            "Epoch 12/20\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - loss: 0.2458\n",
            "Epoch 13/20\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step - loss: 0.2393\n",
            "Epoch 14/20\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - loss: 0.2458\n",
            "Epoch 15/20\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.2411\n",
            "Epoch 16/20\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.2395\n",
            "Epoch 17/20\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.2361\n",
            "Epoch 18/20\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.2371\n",
            "Epoch 19/20\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.2428\n",
            "Epoch 20/20\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.2325\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step\n",
            "Mean Absolute Error: 0.49649640552553476\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip nstall tensorflow xgboost pandas numpy scikit-learn hummingbird-ml\n"
      ],
      "metadata": {
        "id": "dZ653jVSlaNh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Define your model with Input layer\n",
        "model = models.Sequential([\n",
        "    layers.Input(shape=(10,)),  # Input layer\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Save the model in the native Keras format\n",
        "model.save('/content/IsolationForest.keras')\n",
        "\n",
        "# Convert to TensorFlow Lite\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "model_tflite = converter.convert()\n",
        "\n",
        "# Save the TFLite model\n",
        "with open(\"IF.tflite\", \"wb\") as f:\n",
        "    f.write(model_tflite)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUeWCsB-mSgO",
        "outputId": "a7782f5a-b2d6-4bfd-c974-3b3a82a91dca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved artifact at '/tmp/tmpivqoyolo'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 10), dtype=tf.float32, name='keras_tensor_3')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  140164308884880: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  140164308887376: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  140164308885648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  140164308880848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import IsolationForest\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the dataset\n",
        "file_path = \"Dataset Heart Disease.csv\"  # Change path if needed\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Drop unnecessary columns\n",
        "df_cleaned = df.drop(columns=[\"Unnamed: 0\"])\n",
        "\n",
        "# Define features and target\n",
        "X = df_cleaned.drop(columns=[\"target\"])\n",
        "y = df_cleaned[\"target\"].astype(int)  # Ensure y is an integer\n",
        "\n",
        "# Identify numerical columns for scaling\n",
        "num_cols = [\"age\", \"resting bps\", \"cholesterol\", \"max heart rate\", \"oldpeak\"]\n",
        "\n",
        "# Scale numerical features only\n",
        "scaler = StandardScaler()\n",
        "X[num_cols] = scaler.fit_transform(X[num_cols])\n",
        "\n",
        "# Step 2: Split dataset into training (80%) and testing (20%)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert data to NumPy arrays for training compatibility\n",
        "X_train, X_test, y_train, y_test = X_train.values, X_test.values, y_train.values, y_test.values\n",
        "\n",
        "# Step 3: Apply Isolation Forest for Anomaly Detection\n",
        "iso_forest = IsolationForest(n_estimators=200, contamination=0.05, random_state=42)\n",
        "anomaly_scores = iso_forest.fit_predict(X_train)\n",
        "\n",
        "# Identify anomalies (outliers are labeled as -1)\n",
        "anomalies = np.where(anomaly_scores == -1)[0]\n",
        "\n",
        "# Remove detected anomalies from training data to improve accuracy\n",
        "X_train_clean = np.delete(X_train, anomalies, axis=0)\n",
        "y_train_clean = np.delete(y_train, anomalies, axis=0)\n",
        "\n",
        "# Step 4: Train Keras Classifier\n",
        "keras_model = models.Sequential([\n",
        "    layers.Input(shape=(X_train_clean.shape[1],)),  # Input layer with the same shape as the features\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(1, activation='sigmoid')  # Output layer for binary classification\n",
        "])\n",
        "\n",
        "keras_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "keras_model.fit(X_train_clean, y_train_clean, epochs=10, batch_size=32)\n",
        "\n",
        "# Step 5: Evaluate the Keras model\n",
        "y_pred = keras_model.predict(X_test)\n",
        "y_pred_binary = (y_pred > 0.5).astype(int).flatten()\n",
        "\n",
        "overall_accuracy = accuracy_score(y_test, y_pred_binary)\n",
        "print(f\"Overall Model Accuracy (After Removing Anomalies): {overall_accuracy:.2f}\")\n",
        "\n",
        "# Step 6: Convert the Keras model to TensorFlow Lite format\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(keras_model)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the TFLite model to a file\n",
        "with open('isolationforest.tflite', 'wb') as f:\n",
        "    f.write(tflite_model)\n",
        "\n",
        "print(\"TensorFlow Lite model saved as 'model.tflite'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q64cmdSzsOfs",
        "outputId": "bf207356-06c7-470e-85ad-14cf68d23d22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5322 - loss: 0.7176\n",
            "Epoch 2/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6086 - loss: 0.6490\n",
            "Epoch 3/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6415 - loss: 0.6160\n",
            "Epoch 4/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6687 - loss: 0.6038\n",
            "Epoch 5/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7145 - loss: 0.5771\n",
            "Epoch 6/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7295 - loss: 0.5643\n",
            "Epoch 7/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7096 - loss: 0.5712 \n",
            "Epoch 8/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7642 - loss: 0.5336 \n",
            "Epoch 9/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7648 - loss: 0.5198 \n",
            "Epoch 10/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7622 - loss: 0.5348\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
            "Overall Model Accuracy (After Removing Anomalies): 0.72\n",
            "Saved artifact at '/tmp/tmppcl__zj4'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 11), dtype=tf.float32, name='keras_tensor_6')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  140164033433808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  140164033434576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  140164033434192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  140164033433616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "TensorFlow Lite model saved as 'model.tflite'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras import models, layers\n",
        "from sklearn.metrics import accuracy_score\n",
        "import tensorflow as tf\n",
        "\n",
        "# Step 1: Load the dataset\n",
        "file_path = \"Dataset Heart Disease.csv\"  # Adjust path if needed\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Drop unnecessary columns\n",
        "df_cleaned = df.drop(columns=[\"Unnamed: 0\"])\n",
        "\n",
        "# Define features and target\n",
        "X = df_cleaned.drop(columns=[\"target\"])\n",
        "y = df_cleaned[\"target\"].astype(int)  # Ensure target is an integer\n",
        "\n",
        "# Identify numerical columns for scaling\n",
        "num_cols = [\"age\", \"resting bps\", \"cholesterol\", \"max heart rate\", \"oldpeak\"]\n",
        "\n",
        "# Scale numerical features\n",
        "scaler = StandardScaler()\n",
        "X[num_cols] = scaler.fit_transform(X[num_cols])\n",
        "\n",
        "# Step 2: Split dataset into training (80%) and testing (20%)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Create a Keras model for classification\n",
        "model = models.Sequential([\n",
        "    layers.Input(shape=(X_train.shape[1],)),  # Input layer\n",
        "    layers.Dense(64, activation='relu'),  # Hidden layer with 64 units and ReLU activation\n",
        "    layers.Dense(1, activation='sigmoid')  # Output layer for binary classification\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Step 4: Train the Keras model\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
        "\n",
        "# Step 5: Predict on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_binary = (y_pred > 0.5).astype(int).flatten()  # Convert probabilities to binary values\n",
        "\n",
        "# Step 6: Compute overall accuracy and loss\n",
        "accuracy = accuracy_score(y_test, y_pred_binary)\n",
        "accuracy_loss = 1 - accuracy  # Loss is 1 - accuracy\n",
        "\n",
        "# Print overall accuracy and loss with high precision\n",
        "print(f\"Model Accuracy: {accuracy:.5f}\")\n",
        "print(f\"Model Loss: {accuracy_loss:.5f}\")\n",
        "\n",
        "# Step 7: Disease mapping (adjust based on your dataset)\n",
        "# Mapping numeric target values to actual disease names\n",
        "disease_mapping = {\n",
        "    0: \"Heart Attack\",\n",
        "    1: \"Heart Failure\",\n",
        "    2: \"Congenital Heart Disease\",\n",
        "    3: \"Arrhythmia\"  # Add any other disease types if necessary\n",
        "}\n",
        "\n",
        "# Map actual and predicted target values to disease names\n",
        "y_test_mapped = [disease_mapping[val] for val in y_test]\n",
        "y_pred_mapped = [disease_mapping[val] for val in y_pred_binary]\n",
        "\n",
        "# Step 8: Calculate accuracy and loss for each row\n",
        "accuracy_per_row = [1 if actual == predicted else 0 for actual, predicted in zip(y_test, y_pred_binary)]\n",
        "loss_per_row = [1 - acc for acc in accuracy_per_row]\n",
        "\n",
        "# Create a DataFrame for disease names with corresponding accuracy/loss\n",
        "results_df = pd.DataFrame({\n",
        "    'Disease Type': y_pred_mapped,  # Showing predicted disease name\n",
        "    'Accuracy Rate': accuracy_per_row,\n",
        "    'Loss Rate': loss_per_row\n",
        "})\n",
        "\n",
        "# Print all rows without rounding off accuracy and loss rates\n",
        "pd.set_option('display.float_format', '{:.5f}'.format)  # Set precision for display\n",
        "\n",
        "print(\"\\nAll Test Cases with Disease Types, Accuracy, and Loss Rates:\")\n",
        "print(results_df)\n",
        "\n",
        "# Step 9: Convert the Keras model to TensorFlow Lite\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the TFLite model to a file\n",
        "with open('SVM.tflite', 'wb') as f:\n",
        "    f.write(tflite_model)\n",
        "\n",
        "print(\"TensorFlow Lite model saved as 'heart_disease_model.tflite'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aV7h_HU9tPu_",
        "outputId": "67eab32f-44d7-49dc-8119-f0351b634ce1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5238 - loss: 0.6883\n",
            "Epoch 2/10\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6407 - loss: 0.6325\n",
            "Epoch 3/10\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6613 - loss: 0.6183\n",
            "Epoch 4/10\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6726 - loss: 0.5986\n",
            "Epoch 5/10\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7089 - loss: 0.5835\n",
            "Epoch 6/10\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7307 - loss: 0.5591\n",
            "Epoch 7/10\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7169 - loss: 0.5649\n",
            "Epoch 8/10\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7387 - loss: 0.5480\n",
            "Epoch 9/10\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7704 - loss: 0.5336\n",
            "Epoch 10/10\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7773 - loss: 0.5308\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "Model Accuracy: 0.73810\n",
            "Model Loss: 0.26190\n",
            "\n",
            "All Test Cases with Disease Types, Accuracy, and Loss Rates:\n",
            "      Disease Type  Accuracy Rate  Loss Rate\n",
            "0    Heart Failure              1          0\n",
            "1     Heart Attack              0          1\n",
            "2     Heart Attack              1          0\n",
            "3     Heart Attack              1          0\n",
            "4     Heart Attack              1          0\n",
            "..             ...            ...        ...\n",
            "205  Heart Failure              1          0\n",
            "206  Heart Failure              1          0\n",
            "207  Heart Failure              1          0\n",
            "208  Heart Failure              1          0\n",
            "209  Heart Failure              1          0\n",
            "\n",
            "[210 rows x 3 columns]\n",
            "Saved artifact at '/tmp/tmpab3ys812'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 11), dtype=tf.float32, name='keras_tensor_9')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  140164308878736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  140164308881808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  140164333186576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  140164333190416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "TensorFlow Lite model saved as 'heart_disease_model.tflite'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('/content/Dataset Heart Disease.csv')\n",
        "\n",
        "# Print the column names to ensure correct referencing\n",
        "print(df.columns)\n",
        "\n",
        "# Normalize the features (replace these with your actual column names)\n",
        "features = [\"age\", \"resting bps\", \"cholesterol\", \"max heart rate\", \"oldpeak\"]\n",
        "target = 'target'  # Target column for prediction\n",
        "\n",
        "# Normalize the features and the target separately\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "scaled_features = scaler.fit_transform(df[features])\n",
        "\n",
        "# Normalize the target separately\n",
        "scaler_target = MinMaxScaler(feature_range=(0, 1))\n",
        "scaled_target = scaler_target.fit_transform(df[[target]])\n",
        "\n",
        "# Prepare data for LSTM (assuming the time series data is already ordered)\n",
        "look_back = 30  # Number of previous time steps to look at (e.g., last 30 time steps)\n",
        "\n",
        "# Function to create datasets for LSTM\n",
        "def create_dataset(data, target, look_back):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - look_back):\n",
        "        X.append(data[i:(i + look_back)])\n",
        "        y.append(target[i + look_back])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "X, y = create_dataset(scaled_features, scaled_target, look_back)\n",
        "\n",
        "# Reshape the data for LSTM input (samples, time steps, features)\n",
        "X = X.reshape(X.shape[0], X.shape[1], X.shape[2])\n",
        "\n",
        "# Step 2: Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
        "\n",
        "# Step 3: Build the LSTM Model\n",
        "model = Sequential()\n",
        "model.add(LSTM(50, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
        "model.add(LSTM(50, return_sequences=False))\n",
        "model.add(Dense(25))\n",
        "model.add(Dense(1))  # Predicting one value (e.g., target)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Step 4: Train the Model\n",
        "model.fit(X_train, y_train, epochs=5, batch_size=32)\n",
        "\n",
        "# Step 5: Evaluate the Model\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "# Inverse transform the predictions (only for the target)\n",
        "predictions = scaler_target.inverse_transform(predictions)\n",
        "\n",
        "# Inverse transform the actual values (only for the target)\n",
        "y_test_actual = scaler_target.inverse_transform(y_test)\n",
        "\n",
        "# Example: Calculate Mean Absolute Error\n",
        "mae = mean_absolute_error(y_test_actual, predictions)\n",
        "print(f'Mean Absolute Error: {mae}')\n",
        "\n",
        "# Step 6: Convert the trained model to TensorFlow Lite format\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.allow_custom_ops = True\n",
        "# Enable resource variables and set supported operations\n",
        "converter.experimental_enable_resource_variables = True\n",
        "converter.target_spec.supported_ops = [\n",
        "    tf.lite.OpsSet.TFLITE_BUILTINS,\n",
        "    tf.lite.OpsSet.SELECT_TF_OPS\n",
        "]\n",
        "\n",
        "# Perform the conversion\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Step 7: Save the TensorFlow Lite model to a file\n",
        "with open('lstm_model.tflite', 'wb') as f:\n",
        "    f.write(tflite_model)\n",
        "\n",
        "print(\"TensorFlow Lite model saved as 'lstm_model.tflite'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VWfhCDhbthLt",
        "outputId": "112ca234-2f9c-46af-ba8b-58b9d3d2f75f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['Unnamed: 0', 'age', 'sex', 'chest pain type', 'resting bps',\n",
            "       'cholesterol', 'fasting blood sugar', 'resting ecg', 'max heart rate',\n",
            "       'exercise angina', 'oldpeak', 'ST slope', 'target'],\n",
            "      dtype='object')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - loss: 0.3007\n",
            "Epoch 2/5\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.2448\n",
            "Epoch 3/5\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.2387\n",
            "Epoch 4/5\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.2449\n",
            "Epoch 5/5\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.2389\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 15 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7814a5e5f600> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step\n",
            "Mean Absolute Error: 0.5002403171623454\n",
            "Saved artifact at '/tmp/tmpfzrdqhne'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 30, 5), dtype=tf.float32, name='keras_tensor_22')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  132030081302352: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132030081300240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132030081299664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132030081300048: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132030081298896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132030081298512: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132030081299472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132030081301200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132030081298704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132030099834448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "TensorFlow Lite model saved as 'lstm_model.tflite'\n"
          ]
        }
      ]
    }
  ]
}